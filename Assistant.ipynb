{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-21 20:36:19.896 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import torch.nn.functional as f\n",
    "from langchain import LLMChain\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from dotenv import load_dotenv ,find_dotenv\n",
    "import streamlit as st\n",
    "apikey = \"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\"\n",
    "st.title('I am your assintant to help you create your content')\n",
    "from langchain.llms import HuggingFaceHub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv , find_dotenv\n",
    "import os \n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "llm = OpenAI(temperature= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe capital of India is New Delhi.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict('what is capital of india')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for HuggingFaceHub\nmodel\n  extra fields not permitted (type=value_error.extra)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m HuggingFaceHub( model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgoogle/flan-t5-base\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/load/serializable.py:107\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for HuggingFaceHub\nmodel\n  extra fields not permitted (type=value_error.extra)"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceHub( model='google/flan-t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/t5-v1_1-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2798935828.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    PromptTemplate = PromptTemplate(input_variables=)\u001b[0m\n\u001b[0m                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "PromptTemplate = PromptTemplate(input_variables=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "/Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delhi is the capital city of India. Delhi is the capital city of the country. So,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"'questions\": 'Who is modi', 'text': 'Modi is a Hindu guru. Modi is the founder of Modi. So'}\n",
      "Delhi is a city in Rajasthan. The answer: Rajasthan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'\n",
    "from langchain import HuggingFaceHub, PromptTemplate , LLMChain\n",
    "from transformers import pipeline\n",
    "template = \"\"\"  Question :{'questions}\n",
    "Answer : Lets think step by step\"\"\"\n",
    "prompt = PromptTemplate(template = template , input_variables= {'questions'})\n",
    "llmchain = LLMChain(prompt = prompt , llm= HuggingFaceHub(repo_id= \"google/flan-t5-base\"))\n",
    "questions = 'What is capital of india'\n",
    "print(llmchain.run(questions))\n",
    "print(llmchain('Who is modi'))\n",
    "print(llmchain.run('Where is delhi'))\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM,pipeline,AutoModelForSeq2SeqLM\n",
    "model_id = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "from langchain import HuggingFaceHub , PromptTemplate, LLMChain\n",
    "from transformers import pipeline\n",
    "template = \"\"\" Questions : {'questions}\n",
    "Answer: Lets think step by step\"\"\"\n",
    "prompt = PromptTemplate(template = template ,input_variables={'questions'})\n",
    "#prompt.format(language = \"hindi\")\n",
    "llmchain = LLMChain(prompt = prompt , llm= HuggingFaceHub(repo_id= \"google/flan-t5-base\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delhi is the capital city of India. Delhi is the capital city of the country. So,\n",
      "{\"'questions\": 'Who is modi', 'text': 'Modi is a Hindu guru. Modi is the founder of Modi. So'}\n",
      "Delhi is a city in Rajasthan. The answer: Rajasthan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from lanchain import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Assistant is a term used to refer to a person who is a member of '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmchain.run('what is name of assistant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_2 = \"\"\" {'Title' : title \n",
    "Answer : Here is your title for this project}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template = template_2 , input_variables= {'Title'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3360096582.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    llmchain_2 = LLMChain(prompt = prompt ,llm = huggingface_hub(repo_id =))\u001b[0m\n\u001b[0m                                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "llmchain_2 = LLMChain(prompt = prompt ,llm = huggingface_hub(repo_id =))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BigGAN' from 'transformers' (/Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2Tokenizer, GPT2Model, BigGAN\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Initializing a GPT2 tokenizer and model\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'BigGAN' from 'transformers' (/Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model, BigGAN\n",
    "import torch\n",
    "\n",
    "# Initializing a GPT2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "# Tokenizing input text\n",
    "inputs = tokenizer(\"Translate this text into an image\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Using last_hidden_states\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "# truncate and reshape the tokens for BigGAN\n",
    "tokens = last_hidden_states[:, :128, :].reshape(1, -1, 128)\n",
    "\n",
    "# Generate an image from last layer of GPT-2 using BigGAN\n",
    "biggan = BigGAN.from_pretrained('biggan-deep-512')\n",
    "output = biggan(tokens, truncation=0.4)\n",
    "\n",
    "# convert output tensor to a PIL Image and display\n",
    "output_image = torch.nn.functional.to_pil_image(output[0].clamp(min=-1, max=1), mode='RGB')\n",
    "output_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_pretrained_biggan'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_pretrained_biggan\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (BigGAN, one_hot_from_names, truncated_noise_sample)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load pre-trained model tokenizer (vocabulary)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m BigGAN\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbiggan-deep-512\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_pretrained_biggan'"
     ]
    }
   ],
   "source": [
    "from pytorch_pretrained_biggan import (BigGAN, one_hot_from_names, truncated_noise_sample)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "model = BigGAN.from_pretrained('biggan-deep-512')\n",
    "\n",
    "# Prepare a input\n",
    "truncation = 0.4\n",
    "class_vector = one_hot_from_names(['soap bubble'], batch_size=1)\n",
    "noise_vector = truncated_noise_sample(truncation=truncation, batch_size=1)\n",
    "\n",
    "# All in tensors\n",
    "noise_vector = torch.from_numpy(noise_vector)\n",
    "class_vector = torch.from_numpy(class_vector)\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "noise_vector = noise_vector.to(device)\n",
    "class_vector = class_vector.to(device)\n",
    "model.to(device)\n",
    "\n",
    "# Generate an image\n",
    "with torch.no_grad():\n",
    "    output = model(noise_vector, class_vector, truncation)\n",
    "\n",
    "# If you have a GPU put back on CPU\n",
    "output = output.to('cpu')\n",
    "\n",
    "# If you have a sixtel compatible color space, convert the image to 0-255 values.\n",
    "output = output.clamp(min=-1, max=1)\n",
    "\n",
    "output_image = torch.nn.functional.to_pil_image(output[0], mode='RGB')\n",
    "output_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'input_keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m chains \u001b[38;5;241m=\u001b[39m SequentialChain(chains\u001b[38;5;241m=\u001b[39m llmchain, input_variables\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestions\u001b[39m\u001b[38;5;124m'\u001b[39m], output_variables\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestions\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnswers\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/load/serializable.py:107\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pydantic/v1/main.py:339\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mCreate a new model by parsing and validating input data from keyword arguments.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03mRaises ValidationError if the input data cannot be parsed to form a valid model.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pydantic/v1/main.py:1050\u001b[0m, in \u001b[0;36mvalidate_model\u001b[0;34m(model, input_data, cls)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m validator \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39m__pre_root_validators__:\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1050\u001b[0m         input_data \u001b[38;5;241m=\u001b[39m validator(cls_, input_data)\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAssertionError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m   1052\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {}, \u001b[38;5;28mset\u001b[39m(), ValidationError([ErrorWrapper(exc, loc\u001b[38;5;241m=\u001b[39mROOT_KEY)], cls_)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain/chains/sequential.py:64\u001b[0m, in \u001b[0;36mSequentialChain.validate_chains\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m     61\u001b[0m known_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(input_variables \u001b[38;5;241m+\u001b[39m memory_keys)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chain \u001b[38;5;129;01min\u001b[39;00m chains:\n\u001b[0;32m---> 64\u001b[0m     missing_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(chain\u001b[38;5;241m.\u001b[39minput_keys)\u001b[38;5;241m.\u001b[39mdifference(known_variables)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chain\u001b[38;5;241m.\u001b[39mmemory:\n\u001b[1;32m     66\u001b[0m         missing_vars \u001b[38;5;241m=\u001b[39m missing_vars\u001b[38;5;241m.\u001b[39mdifference(chain\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mmemory_variables)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'input_keys'"
     ]
    }
   ],
   "source": [
    "chains = SequentialChain(chains= llmchain, input_variables= ['questions'], output_variables= ['questions','Answers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tajmahal is a village in the District of Bahruzh'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmchain.run('where is tajmahal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating chat bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import HumanInputChatModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage ,AIMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChatOpenAI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m chat_message \u001b[38;5;241m=\u001b[39m ChatOpenAI(prompt \u001b[38;5;241m=\u001b[39m prompt , llm\u001b[38;5;241m=\u001b[39m HuggingFaceHub(repo_id\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/flan-t5-base\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ChatOpenAI' is not defined"
     ]
    }
   ],
   "source": [
    "chat_message = ChatOpenAI(prompt = prompt , llm= HuggingFaceHub(repo_id= \"google/flan-t5-base\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (3136133015.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[27], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    SystemMessage(content='You are content creater assinstant please help in creating content in 1000 words')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "chat_message([\n",
    "    SystemMessage(content='You are content creater assinstant please help in creating content in 1000 words')\n",
    " HumanMessage(content=' Please provide complete script of a video')\n",
    " \n",
    " \n",
    " \n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! repo_id is not default parameter.\n",
      "                    repo_id was transferred to model_kwargs.\n",
      "                    Please confirm that repo_id is what you intended.\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"  Question :{'questions}\n",
    "Answer : Lets think step by step\"\"\"\n",
    "prompt = PromptTemplate(template = template , input_variables= {'questions'})\n",
    "llmchain = LLMChain(prompt = prompt , llm= ChatOpenAI(repo_id=\"gpt-3.5-turbo-instruct\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LLMChain' object has no attribute 'PromptTemplate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llmchain\u001b[38;5;241m.\u001b[39mPromptTemplate(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhere is india\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LLMChain' object has no attribute 'PromptTemplate'"
     ]
    }
   ],
   "source": [
    "llmchain.PromptTemplate('where is india')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: 'tuple' object is not callable; perhaps you missed a comma?\n",
      "<>:4: SyntaxWarning: 'tuple' object is not callable; perhaps you missed a comma?\n",
      "/var/folders/qg/ssrxdx1x2wd8frrb53jy7yzc0000gn/T/ipykernel_71348/1813260429.py:4: SyntaxWarning: 'tuple' object is not callable; perhaps you missed a comma?\n",
      "  (\"system\",template)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are help ful for cintent creation script write for youtuber and you need help in makking videos\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m human_template \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNext\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m      3\u001b[0m chat_prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages([\n\u001b[0;32m----> 4\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m,template)\n\u001b[1;32m      5\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m,human_template)\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m ])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "template = \"You are help ful for cintent creation script write for youtuber and you need help in makking videos\"\n",
    "human_template = {'Next'}\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",template)\n",
    "    (\"human\",human_template)\n",
    "\n",
    "\n",
    "\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q --upgrade google-generativeai langchain-google-genai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "import textwrap\n",
    "\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "str expected, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGOOGLE_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGOOGLE_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m<frozen os>:684\u001b[0m, in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n",
      "File \u001b[0;32m<frozen os>:758\u001b[0m, in \u001b[0;36mencode\u001b[0;34m(value)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: str expected, not NoneType"
     ]
    }
   ],
   "source": [
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(model_name = \"gemini-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(model_name = \"gemini-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "DefaultCredentialsError",
     "evalue": "Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDefaultCredentialsError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate_content(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWho is modi\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/google/generativeai/generative_models.py:241\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(\n\u001b[1;32m    235\u001b[0m     contents\u001b[38;5;241m=\u001b[39mcontents,\n\u001b[1;32m    236\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[1;32m    237\u001b[0m     safety_settings\u001b[38;5;241m=\u001b[39msafety_settings,\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    239\u001b[0m )\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mget_default_generative_client()\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mrewrite_stream_error():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/google/generativeai/client.py:230\u001b[0m, in \u001b[0;36mget_default_generative_client\u001b[0;34m()\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_default_generative_client\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m glm\u001b[38;5;241m.\u001b[39mGenerativeServiceClient:\n\u001b[0;32m--> 230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _client_manager\u001b[38;5;241m.\u001b[39mget_default_client(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerative\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/google/generativeai/client.py:161\u001b[0m, in \u001b[0;36m_ClientManager.get_default_client\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    159\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclients\u001b[38;5;241m.\u001b[39mget(name)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_client(name)\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclients[name] \u001b[38;5;241m=\u001b[39m client\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m client\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/google/generativeai/client.py:121\u001b[0m, in \u001b[0;36m_ClientManager.make_client\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_config:\n\u001b[1;32m    119\u001b[0m     configure()\n\u001b[0;32m--> 121\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_config)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_metadata:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m client\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:433\u001b[0m, in \u001b[0;36mGenerativeServiceClient.__init__\u001b[0;34m(self, credentials, transport, client_options, client_info)\u001b[0m\n\u001b[1;32m    428\u001b[0m     credentials \u001b[38;5;241m=\u001b[39m google\u001b[38;5;241m.\u001b[39mauth\u001b[38;5;241m.\u001b[39m_default\u001b[38;5;241m.\u001b[39mget_api_key_credentials(\n\u001b[1;32m    429\u001b[0m         api_key_value\n\u001b[1;32m    430\u001b[0m     )\n\u001b[1;32m    432\u001b[0m Transport \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mget_transport_class(transport)\n\u001b[0;32m--> 433\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport \u001b[38;5;241m=\u001b[39m Transport(\n\u001b[1;32m    434\u001b[0m     credentials\u001b[38;5;241m=\u001b[39mcredentials,\n\u001b[1;32m    435\u001b[0m     credentials_file\u001b[38;5;241m=\u001b[39mclient_options\u001b[38;5;241m.\u001b[39mcredentials_file,\n\u001b[1;32m    436\u001b[0m     host\u001b[38;5;241m=\u001b[39mapi_endpoint,\n\u001b[1;32m    437\u001b[0m     scopes\u001b[38;5;241m=\u001b[39mclient_options\u001b[38;5;241m.\u001b[39mscopes,\n\u001b[1;32m    438\u001b[0m     client_cert_source_for_mtls\u001b[38;5;241m=\u001b[39mclient_cert_source_func,\n\u001b[1;32m    439\u001b[0m     quota_project_id\u001b[38;5;241m=\u001b[39mclient_options\u001b[38;5;241m.\u001b[39mquota_project_id,\n\u001b[1;32m    440\u001b[0m     client_info\u001b[38;5;241m=\u001b[39mclient_info,\n\u001b[1;32m    441\u001b[0m     always_use_jwt_access\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    442\u001b[0m     api_audience\u001b[38;5;241m=\u001b[39mclient_options\u001b[38;5;241m.\u001b[39mapi_audience,\n\u001b[1;32m    443\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:150\u001b[0m, in \u001b[0;36mGenerativeServiceGrpcTransport.__init__\u001b[0;34m(self, host, credentials, credentials_file, scopes, channel, api_mtls_endpoint, client_cert_source, ssl_channel_credentials, client_cert_source_for_mtls, quota_project_id, client_info, always_use_jwt_access, api_audience)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ssl_channel_credentials \u001b[38;5;241m=\u001b[39m grpc\u001b[38;5;241m.\u001b[39mssl_channel_credentials(\n\u001b[1;32m    146\u001b[0m                 certificate_chain\u001b[38;5;241m=\u001b[39mcert, private_key\u001b[38;5;241m=\u001b[39mkey\n\u001b[1;32m    147\u001b[0m             )\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# The base transport sets the host, credentials and scopes\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    151\u001b[0m     host\u001b[38;5;241m=\u001b[39mhost,\n\u001b[1;32m    152\u001b[0m     credentials\u001b[38;5;241m=\u001b[39mcredentials,\n\u001b[1;32m    153\u001b[0m     credentials_file\u001b[38;5;241m=\u001b[39mcredentials_file,\n\u001b[1;32m    154\u001b[0m     scopes\u001b[38;5;241m=\u001b[39mscopes,\n\u001b[1;32m    155\u001b[0m     quota_project_id\u001b[38;5;241m=\u001b[39mquota_project_id,\n\u001b[1;32m    156\u001b[0m     client_info\u001b[38;5;241m=\u001b[39mclient_info,\n\u001b[1;32m    157\u001b[0m     always_use_jwt_access\u001b[38;5;241m=\u001b[39malways_use_jwt_access,\n\u001b[1;32m    158\u001b[0m     api_audience\u001b[38;5;241m=\u001b[39mapi_audience,\n\u001b[1;32m    159\u001b[0m )\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grpc_channel:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grpc_channel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreate_channel(\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host,\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# use the credentials which are saved\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    175\u001b[0m         ],\n\u001b[1;32m    176\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py:98\u001b[0m, in \u001b[0;36mGenerativeServiceTransport.__init__\u001b[0;34m(self, host, credentials, credentials_file, scopes, quota_project_id, client_info, always_use_jwt_access, api_audience, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m     credentials, _ \u001b[38;5;241m=\u001b[39m google\u001b[38;5;241m.\u001b[39mauth\u001b[38;5;241m.\u001b[39mload_credentials_from_file(\n\u001b[1;32m     95\u001b[0m         credentials_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mscopes_kwargs, quota_project_id\u001b[38;5;241m=\u001b[39mquota_project_id\n\u001b[1;32m     96\u001b[0m     )\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m credentials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m     credentials, _ \u001b[38;5;241m=\u001b[39m google\u001b[38;5;241m.\u001b[39mauth\u001b[38;5;241m.\u001b[39mdefault(\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mscopes_kwargs, quota_project_id\u001b[38;5;241m=\u001b[39mquota_project_id\n\u001b[1;32m    100\u001b[0m     )\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# Don't apply audience if the credentials file passed from user.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(credentials, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith_gdch_audience\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/google/auth/_default.py:691\u001b[0m, in \u001b[0;36mdefault\u001b[0;34m(scopes, request, quota_project_id, default_scopes)\u001b[0m\n\u001b[1;32m    683\u001b[0m             _LOGGER\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    684\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo project ID could be determined. Consider running \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    685\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`gcloud config set project` or setting the \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    686\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menvironment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    687\u001b[0m                 environment_vars\u001b[38;5;241m.\u001b[39mPROJECT,\n\u001b[1;32m    688\u001b[0m             )\n\u001b[1;32m    689\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m credentials, effective_project_id\n\u001b[0;32m--> 691\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mDefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "\u001b[0;31mDefaultCredentialsError\u001b[0m: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information."
     ]
    }
   ],
   "source": [
    "response = model.generate_content('Who is modi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mtext)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_API_KEY'] = 'AIzaSyAfzR96mRJRQX2kcVK8BBB7AerWBvbf-sc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> **Comprehensive Fitness Routine:**\n",
       "> \n",
       "> **1. Regular Cardiovascular Exercise:**\n",
       "> - Aim for 150 minutes of moderate-intensity or 75 minutes of vigorous-intensity exercise per week.\n",
       "> - Examples: brisk walking, jogging, swimming, cycling\n",
       "> \n",
       "> **2. Resistance Training:**\n",
       "> - Engage in muscle-strengthening exercises 2-3 times per week.\n",
       "> - Include exercises like squats, push-ups, and lunges.\n",
       "> - Use weights or bodyweight resistance.\n",
       "> \n",
       "> **3. Flexibility and Balance:**\n",
       "> - Perform stretching or yoga exercises several times per week.\n",
       "> - Improve posture, reduce muscle soreness, and maintain mobility.\n",
       "> \n",
       "> **4. Core Strengthening:**\n",
       "> - Focus on exercises that engage the abdominal and back muscles.\n",
       "> - Examples: planks, crunches, and bridges.\n",
       "> \n",
       "> **5. Rest and Recovery:**\n",
       "> - Allow your body adequate time to rest and recover between workouts.\n",
       "> - Get 7-9 hours of sleep per night.\n",
       "> \n",
       "> **Lifestyle Habits:**\n",
       "> \n",
       "> **1. Healthy Diet:**\n",
       "> - Consume a balanced diet rich in fruits, vegetables, whole grains, and lean protein.\n",
       "> - Limit processed foods, sugary drinks, and unhealthy fats.\n",
       "> \n",
       "> **2. Hydration:**\n",
       "> - Drink plenty of water throughout the day, especially before and after workouts.\n",
       "> \n",
       "> **3. Sleep:**\n",
       "> - Establish a regular sleep schedule and create a conducive sleep environment.\n",
       "> \n",
       "> **4. Stress Management:**\n",
       "> - Engage in stress-reducing activities such as exercise, meditation, or yoga.\n",
       "> \n",
       "> **5. Consistency:**\n",
       "> - Make fitness a regular part of your routine, even when you don't feel motivated.\n",
       "> \n",
       "> **Additional Tips:**\n",
       "> \n",
       "> * **Set Realistic Goals:** Start gradually and increase intensity and duration over time.\n",
       "> * **Find Activities You Enjoy:** Choose exercises that you find fun and engaging.\n",
       "> * **Listen to Your Body:** Pay attention to your body's signals and rest when necessary.\n",
       "> * **Consult with a Healthcare Professional:** If you have any underlying health conditions, consult with your doctor before starting a fitness routine.\n",
       "> * **Consider a Personal Trainer:** A trainer can provide personalized guidance and support.\n",
       "> * **Stay Motivated:** Surround yourself with supportive people, set small rewards for yourself, and focus on the benefits of exercise."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "result = llm.invoke(\"What is the best practice to keep fit?\")\n",
    "to_markdown(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke('what is best way to fine tune a llm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Best Practices for Fine-Tuning a Large Language Model (LLM)**\n",
      "\n",
      "**1. Define a Clear Fine-Tuning Goal:**\n",
      "* Determine the specific task or domain you want the fine-tuned LLM to excel at.\n",
      "* Establish clear metrics to evaluate the fine-tuned model's performance.\n",
      "\n",
      "**2. Gather and Prepare High-Quality Data:**\n",
      "* Collect a large and diverse dataset relevant to the fine-tuning goal.\n",
      "* Clean and preprocess the data to ensure it is free of errors and biases.\n",
      "\n",
      "**3. Choose an Appropriate Fine-Tuning Architecture:**\n",
      "* Select an LLM architecture that is suitable for the task at hand.\n",
      "* Consider the LLM's size, training objectives, and desired output format.\n",
      "\n",
      "**4. Set Optimal Fine-Tuning Parameters:**\n",
      "* Define the number of training epochs, batch size, learning rate, and other hyperparameters.\n",
      "* Use a validation set to optimize these parameters for maximum performance.\n",
      "\n",
      "**5. Utilize Transfer Learning:**\n",
      "* Start with a pre-trained LLM that has been trained on a general corpus.\n",
      "* Fine-tune the LLM on your specific dataset to leverage its existing knowledge.\n",
      "\n",
      "**6. Use Prompt Engineering Techniques:**\n",
      "* Craft effective prompts that guide the LLM's behavior and improve task performance.\n",
      "* Experiment with different prompt formats, lengths, and content.\n",
      "\n",
      "**7. Incorporate Domain Knowledge:**\n",
      "* Embed domain-specific knowledge into the fine-tuning process through additional data sources or constraints.\n",
      "* Use expert-curated datasets or knowledge graphs to enhance the LLM's understanding.\n",
      "\n",
      "**8. Monitor and Evaluate Performance:**\n",
      "* Track the LLM's performance on both the training and validation sets during fine-tuning.\n",
      "* Make adjustments to the fine-tuning parameters or training data as needed.\n",
      "\n",
      "**9. Post-Processing and Deployment:**\n",
      "* Apply post-processing techniques to further improve the fine-tuned LLM's output.\n",
      "* Integrate the fine-tuned LLM into your application or system for practical use.\n",
      "\n",
      "**10. Continuous Improvement:**\n",
      "* Monitor the fine-tuned LLM's performance over time.\n",
      "* Collect feedback and make adjustments to the fine-tuning process as necessary to maintain optimal performance.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader, VectorStoreIndex, ServiceContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "#documents = SimpleDirectoryReader(\"\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_index import download_loader\n",
    "\n",
    "PDFReader = download_loader(\"PDFReader\")\n",
    "\n",
    "loader = PDFReader()\n",
    "#documents = loader.load_data(file=Path(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-embeddings-google\n",
      "  Downloading llama_index_embeddings_google-0.1.3-py3-none-any.whl (4.6 kB)\n",
      "Requirement already satisfied: google-generativeai<0.4.0,>=0.3.2 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from llama-index-embeddings-google) (0.3.2)\n",
      "Collecting llama-index-core<0.11.0,>=0.10.11.post1 (from llama-index-embeddings-google)\n",
      "  Downloading llama_index_core-0.10.11.post1-py3-none-any.whl (15.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-ai-generativelanguage==0.4.0 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from google-generativeai<0.4.0,>=0.3.2->llama-index-embeddings-google) (0.4.0)\n",
      "Requirement already satisfied: google-auth in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from google-generativeai<0.4.0,>=0.3.2->llama-index-embeddings-google) (2.27.0)\n",
      "Requirement already satisfied: google-api-core in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from google-generativeai<0.4.0,>=0.3.2->llama-index-embeddings-google) (2.17.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from google-generativeai<0.4.0,>=0.3.2->llama-index-embeddings-google) (4.9.0)\n",
      "Requirement already satisfied: protobuf in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from google-generativeai<0.4.0,>=0.3.2->llama-index-embeddings-google) (4.25.2)\n",
      "Requirement already satisfied: tqdm in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from google-generativeai<0.4.0,>=0.3.2->llama-index-embeddings-google) (4.65.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from google-ai-generativelanguage==0.4.0->google-generativeai<0.4.0,>=0.3.2->llama-index-embeddings-google) (1.23.0)\n",
      "Collecting PyYAML>=6.0.1 (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google)\n",
      "  Downloading PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl (167 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.5/167.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (0.6.3)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (2023.12.2)\n",
      "Requirement already satisfied: httpx in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (0.25.2)\n",
      "Collecting llamaindex-py-client<0.2.0,>=0.1.13 (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google)\n",
      "  Downloading llamaindex_py_client-0.1.13-py3-none-any.whl (107 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (3.2.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (3.8.1)\n",
      "Requirement already satisfied: numpy in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (1.25.2)\n",
      "Requirement already satisfied: openai>=1.1.0 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (1.8.0)\n",
      "Requirement already satisfied: pandas in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (1.5.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (9.4.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (8.2.2)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (0.5.2)\n",
      "Collecting tqdm (from google-generativeai<0.4.0,>=0.3.2->llama-index-embeddings-google)\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-inspect>=0.8.0 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (0.9.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (1.8.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (1.14.1)\n",
      "Requirement already satisfied: pydantic>=1.10 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (2.4.2)\n",
      "Requirement already satisfied: anyio in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (3.5.0)\n",
      "Requirement already satisfied: certifi in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (2023.5.7)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (1.0.2)\n",
      "Requirement already satisfied: idna in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (3.4)\n",
      "Requirement already satisfied: sniffio in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (1.2.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (0.14.0)\n",
      "Requirement already satisfied: click in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (2022.7.9)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (2.2.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (2.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (0.4.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (3.20.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from google-api-core->google-generativeai<0.4.0,>=0.3.2->llama-index-embeddings-google) (1.62.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from google-auth->google-generativeai<0.4.0,>=0.3.2->llama-index-embeddings-google) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from google-auth->google-generativeai<0.4.0,>=0.3.2->llama-index-embeddings-google) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from google-auth->google-generativeai<0.4.0,>=0.3.2->llama-index-embeddings-google) (4.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (2022.7)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from google-api-core->google-generativeai<0.4.0,>=0.3.2->llama-index-embeddings-google) (1.60.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from google-api-core->google-generativeai<0.4.0,>=0.3.2->llama-index-embeddings-google) (1.60.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (23.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth->google-generativeai<0.4.0,>=0.3.2->llama-index-embeddings-google) (0.4.8)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (2.10.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/arpitagnihotri/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-google) (1.16.0)\n",
      "Installing collected packages: tqdm, PyYAML, llamaindex-py-client, llama-index-core, llama-index-embeddings-google\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.65.0\n",
      "    Uninstalling tqdm-4.65.0:\n",
      "      Successfully uninstalled tqdm-4.65.0\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "      Successfully uninstalled PyYAML-6.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.41 requires requests_mock, which is not installed.\n",
      "conda-repo-cli 1.0.41 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.41 requires nbformat==5.4.0, but you have nbformat 5.7.0 which is incompatible.\n",
      "conda-repo-cli 1.0.41 requires PyYAML==6.0, but you have pyyaml 6.0.1 which is incompatible.\n",
      "conda-repo-cli 1.0.41 requires requests==2.28.1, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyYAML-6.0.1 llama-index-core-0.10.11.post1 llama-index-embeddings-google-0.1.3 llamaindex-py-client-0.1.13 tqdm-4.66.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llama-index-embeddings-google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GoogleGenerativeAIEmbeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embedding \u001b[38;5;241m=\u001b[39m GoogleGenerativeAIEmbeddings(model\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/embedding-001\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GoogleGenerativeAIEmbeddings' is not defined"
     ]
    }
   ],
   "source": [
    "embedding = GoogleGenerativeAIEmbeddings(model= \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vector = embeddings.embed_query(\"hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  llama_index import VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
